# ğŸ§  Personal AI Data Analyst

An interactive **Streamlit-based data analysis application** that allows users to upload datasets, explore them through **guided analysis suggestions**, and optionally ask **custom questions powered by a local LLM (Ollama)** â€” all without sending data to the cloud.

This project is designed to demonstrate practical skills in **Python, data analysis, Streamlit UI development, and local LLM integration**.

---

## âœ¨ Key Features

* ğŸ“ Upload CSV, Excel, or JSON datasets
* ğŸ” Automatic dataset inspection and smart analysis suggestions
* ğŸ“Š Built-in deterministic analyses (EDA, plots, correlations, anomalies)
* ğŸ¤– Optional **local LLM (Ollama)** for custom natural-language queries
* ğŸ“ˆ Supports text output, tables, and visualizations
* ğŸ”’ Runs fully locally (no data leaves your machine)

---

## ğŸ§± Project Architecture

```
Personal-AI-Data-Analyst/
â”‚
â”œâ”€â”€ app.py          # Streamlit UI & interaction logic
â”œâ”€â”€ analyst.py      # Core analysis engine & LLM interface
â”œâ”€â”€ requirements(PDA).txt
â””â”€â”€ README.md
```

### app.py

* Handles Streamlit UI
* File upload and preview
* Prompt selection and execution
* Output rendering (text / table / chart)

### analyst.py

* Dataset loading (CSV, Excel, JSON)
* Column type detection
* Suggested analysis generation
* Prompt-to-code mapping (deterministic)
* Safe execution of generated Python code
* Optional local LLM execution via Ollama CLI

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Create a virtual environment (recommended)

```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements(PDA).txt
```

Or manually:

```bash
pip install streamlit pandas matplotlib numpy scipy openpyxl
```

---

## ğŸ¤– Optional: Enable Local LLM (Ollama)

This app can optionally use a **local LLM** for free-form analysis prompts.

### Install Ollama

Download from:
[https://ollama.com](https://ollama.com)

### Pull a model

```bash
ollama pull llama3.1
```

### Verify installation

```bash
ollama --version
```

In the app sidebar, enable:

```
â˜‘ Use local LLM (ollama)
Model name: llama3.1
```

---

## â–¶ Running the App

```bash
streamlit run app.py
```

Then open the browser URL shown in the terminal.

---

## ğŸ§ª Example Analyses Supported

* Dataset summary (rows, columns, missing values)
* Top categories and counts
* Numeric statistics (mean, std, quartiles)
* Histograms and scatter plots
* Correlation heatmaps
* Monthly time-series aggregation
* Anomaly detection using Z-score

Custom prompts are supported when LLM mode is enabled.

---

## ğŸ” Security & Safety

* Code execution is sandboxed to a limited namespace
* No arbitrary file system access
* No internet calls from the LLM
* All data remains local

---

## ğŸ¯ Learning Outcomes

This project demonstrates:

* Practical Streamlit application development
* Real-world Exploratory Data Analysis (EDA)
* Safe dynamic code execution patterns
* Local LLM integration using CLI tools
* Clean separation of UI and logic layers

---

## ğŸ“Œ Future Enhancements (Planned)

* UI theming (light/dark)
* Chat-style analysis history
* Smarter prompt understanding
* Dataset profiling report export
* Multi-tab dashboard layout

---

## ğŸ“„ License

This project is for educational and portfolio purposes.
You are free to fork and extend it.
